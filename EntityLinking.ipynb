{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial NLP for MIR\n",
    "## Entity Linking and Entity Similarity\n",
    "We are going to use Elvis, an Entity Linking python wrapper for some Entity Linking systems. More info in http://github.com/sergiooramas/elvis\n",
    "\n",
    "To install elvis do pip install elvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import elvis\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From elvis you can use Babelfy, Tagme and DBpedia Spotlight. To use Babelfy or Tagme you need an API key. To use DBpedia Spotlight you can to install a local server.\n",
    "\n",
    "You can obtain a BabelNet API key in http://babelnet.org/register\n",
    "\n",
    "And a Tagme API key in https://tagme.d4science.org/tagme/tagme_help.html\n",
    "\n",
    "DBpedia Spotlight can be downloaded here https://github.com/dbpedia-spotlight/dbpedia-spotlight\n",
    "\n",
    "The API key of Tagme and Babelfy can be configured with the methods set_babelfy_key(\"key\") set_tagme_key(\"key\")\n",
    "\n",
    "The endpoint of the Spotlight server can be set with the method set_spotlight_endpoint(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elvis.set_babelfy_key(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From elvis we can process an entire folder with text files, using the method process_folder(tool, path_to_folder)\n",
    "\n",
    "Tool can be either ['babelfy','spotlight','tagme']\n",
    "\n",
    "A tool can be call to query an array of sentences with methods babelfy(sentences), tagme(sentences), spotlight(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences.append(\"Madonna Louise Ciccone is an American singer.\")\n",
    "\n",
    "linked_text = elvis.babelfy(sentences)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(linked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to process a folder. The path of the folder should be absolute.\n",
    "\n",
    "We are going to process a dataset of 262 artist biographies, coming from the Semantic Artist Similrity dataset http://mtg.upf.edu/download/datasets/semantic-similarity\n",
    "\n",
    "The method process_folder creates a directory \"entities/tool/dataset_name\", in this case it will create the folder \"entities/babelfy/mirex_biographies\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tutorial_folder = '/Users/soramas/dev/nlp-tutorial/'\n",
    "dataset_folder = tutorial_folder + 'sas_dataset/mirex_biographies'\n",
    "output_folder = tutorial_folder + 'entities/mirex_biographies/babelfy'\n",
    "\n",
    "# To use this method you need to have the English tokenizer of NLTK. You can install it from python interpreter: \n",
    "# import nltk\n",
    "# nltk.download()\n",
    "elvis.process_folder('babelfy',dataset_folder,output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elvis provides with a method to compute similarity between artists based on the entities identified in the biographies. The method is called compute_similarity(entities_folder) and receives the folder where the extracted entities are located. It returns a numpy similarity matrix and a list with the artists in the matrix. Then a second method is used to get the top_n most similar entities of every artist top_n(similarity_matrix, artists_index, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entities_folder = tutorial_folder + 'entities/mirex_biographies/babelfy'\n",
    "artists_index = []\n",
    "similarity_matrix, artists_index = elvis.compute_similarity(entities_folder)\n",
    "top = elvis.top_n(similarity_matrix,artists_index,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filenames of the original text files are used as entity name. In this dataset the text filename correspond with the MusicBrainz ID of the artist. The dataset comes with a mapping from MusicBrainz ID to artist names. Thus, we create a dictionary to convert mbids into artist names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbid2name = dict()\n",
    "f = open(tutorial_folder+'sas_dataset/mb2uri_mirex.tsv')\n",
    "for line in f.readlines():\n",
    "    mbid, name, uri = line.strip().split('\\t')\n",
    "    mbid2name[mbid] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the obtained dictionary and the top_n list of most similar artists we visualize which are the top 5 most similar artists of a subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, similars in enumerate(top[:15]):\n",
    "    print mbid2name[artists_index[index]], \": \", \", \".join([mbid2name[s] for s in similars])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Homogenizing\n",
    "Elvis provides a method to homogenize the output of different Enity Linking systems, and add some missing semantic information from DBpedia. The method is called homogenize(tool,entities_folder). The entities to be homogenized should be in the folder entities_folder+tool. It takes some time, but the process can be speed up storing locally the files the DBpedia files used during homogenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_folder = tutorial_folder + 'sample_text/'\n",
    "output_folder = tutorial_folder + 'entities/sample_text/babelfy/'\n",
    "elvis.process_folder('babelfy',dataset_folder,output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_entities_folder = tutorial_folder + 'entities/sample_text/'\n",
    "elvis.homogenize('babelfy',all_entities_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "homogenized=json.load(open(all_entities_folder + \"babelfy_h/madonna.json\"))\n",
    "pp.pprint(homogenized[0]['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
